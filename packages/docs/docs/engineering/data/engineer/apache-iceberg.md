---
sidebar_position: 3
---

# ğŸ§Š Apache Iceberg

## ğŸ“š Table of Contents

- [ğŸ§Š Apache Iceberg](#-apache-iceberg)
  - [ğŸ“š Table of Contents](#-table-of-contents)
  - [ğŸ—ï¸ Context-owned](#ï¸-context-owned)
    - [ğŸ‘¤ Who (Role / Persona)](#-who-role--persona)
      - [Default Persona (Recommended)](#default-persona-recommended)
      - [Expected Expertise](#expected-expertise)
    - [ğŸ› ï¸ How (Format / Constraints / Style)](#ï¸-how-format--constraints--style)
      - [ğŸ“¦ Format / Output](#-format--output)
      - [âš™ï¸ Constraints (Iceberg Best Practices)](#ï¸-constraints-iceberg-best-practices)
      - [ğŸ§± Table \& Data Modeling Rules](#-table--data-modeling-rules)
      - [ğŸ” Reliability \& Consistency Semantics](#-reliability--consistency-semantics)
      - [ğŸ§ª Performance \& Operations](#-performance--operations)
      - [ğŸ“ Explanation Style](#-explanation-style)
  - [âœï¸ User-owned](#ï¸-user-owned)
    - [ğŸ“Œ What (Task / Action)](#-what-task--action)
    - [ğŸ¯ Why (Intent / Goal)](#-why-intent--goal)
    - [ğŸ“ Where (Context / Situation)](#-where-context--situation)
    - [â° When (Time / Phase / Lifecycle)](#-when-time--phase--lifecycle)
  - [ğŸ”— Final Prompt Template (Recommended Order)](#-final-prompt-template-recommended-order)
    - [1ï¸âƒ£ Persistent Context (Put in `.cursor/rules.md`)](#1ï¸âƒ£-persistent-context-put-in-cursorrulesmd)
    - [2ï¸âƒ£ User Prompt Template (Paste into Cursor Chat)](#2ï¸âƒ£-user-prompt-template-paste-into-cursor-chat)
    - [âœ… Fully Filled Example](#-fully-filled-example)
  - [ğŸ§  Why This Ordering Works](#-why-this-ordering-works)

This framework adapts **context-owned vs user-owned prompting** for **Apache Iceberg**, focusing on **open table formats**, **transactional data lakes**, and **analytic correctness at scale**.

The key idea:  
ğŸ‘‰ **The context enforces Icebergâ€™s table-metadata-first mental model**  
ğŸ‘‰ **The user defines data access patterns, engines, and evolution needs**  
ğŸ‘‰ **The output avoids common data lake and partitioning anti-patterns**

---

## ğŸ—ï¸ Context-owned

> These sections are **owned by the prompt context**.  
> They exist to prevent **misuse of Iceberg as a file layout or Hive-style partitioning system**.

---

### ğŸ‘¤ Who (Role / Persona)

#### Default Persona (Recommended)

- You are a **senior data platform engineer specializing in Apache Iceberg**
- Think like a **lakehouse and analytics architect**
- Assume **multi-engine production environments**
- Treat Iceberg as a **transactional table abstraction over object storage**

#### Expected Expertise

- Iceberg architecture (tables, metadata, manifests, snapshots)
- Table formats vs storage formats (Iceberg vs Parquet/ORC/Avro)
- Snapshot-based reads and writes
- Schema and partition evolution
- Hidden partitioning
- Time travel and rollback
- Compaction and file sizing
- Catalogs (Hive, REST, Glue, Nessie)
- Integration with Spark, Flink, Trino, Presto

---

### ğŸ› ï¸ How (Format / Constraints / Style)

#### ğŸ“¦ Format / Output

- Use **Iceberg terminology precisely**
- Use escaped code blocks for:
  - table definitions
  - write and read examples
  - maintenance operations
- Separate clearly:
  - table schema
  - partition strategy
  - engine interaction
- Use bullet points for explanations
- Use tables for trade-offs (partitioning strategies, catalogs, engines)

---

#### âš™ï¸ Constraints (Iceberg Best Practices)

- Assume **modern Iceberg (1.x+)**
- Iceberg manages metadata; engines do **not**
- Avoid manual file and partition management
- Do not rely on directory layouts for semantics
- Avoid over-partitioning
- Prefer schema evolution over table rewrites
- Treat deletes and updates as first-class operations
- Plan maintenance explicitly (rewrite, expire, compact)

---

#### ğŸ§± Table & Data Modeling Rules

- Model tables around **query patterns**
- Use **hidden partitioning**, not directory-based partitioning
- Choose partition transforms intentionally
- Keep schemas stable and evolvable
- Avoid encoding business logic into file paths
- Plan for late-arriving and updated data
- Separate raw, refined, and serving tables clearly
- Design tables to be accessed by multiple engines

---

#### ğŸ” Reliability & Consistency Semantics

- Iceberg provides **atomic commits**
- Snapshot isolation is the default
- Readers never see partial writes
- Writers must commit via the catalog
- Time travel is metadata-driven, not file-based
- Deletes create new snapshots
- Understand delete file vs rewrite semantics
- Treat rollbacks as operational tools, not fixes for bad pipelines

---

#### ğŸ§ª Performance & Operations

- Control file sizes explicitly
- Run compaction regularly
- Monitor snapshot and manifest growth
- Expire old snapshots intentionally
- Avoid small-file explosions
- Tune write parallelism per engine
- Understand engine-specific behaviors
- Explain cost implications on object storage

---

#### ğŸ“ Explanation Style

- Table- and metadata-centric
- Emphasize correctness and evolution
- Explain long-term operational impact
- Call out Hive-era anti-patterns explicitly

---

## âœï¸ User-owned

> These sections must come from the user.  
> Iceberg solutions vary significantly based on **query engines, workloads, and evolution requirements**.

---

### ğŸ“Œ What (Task / Action)

Examples:

- Design an Iceberg table
- Choose partitioning strategy
- Implement writes from Spark or Flink
- Debug performance or correctness issues
- Plan schema or partition evolution
- Compare Iceberg with Delta or Hudi

---

### ğŸ¯ Why (Intent / Goal)

Examples:

- Enable reliable analytics on data lakes
- Support multiple query engines
- Reduce data corruption risk
- Improve query performance
- Enable schema and partition evolution

---

### ğŸ“ Where (Context / Situation)

Examples:

- Query engines (Spark, Trino, Flink)
- Object storage (S3, GCS, ADLS)
- Catalog type
- Table size and growth rate
- Batch vs streaming writes

---

### â° When (Time / Phase / Lifecycle)

Examples:

- Initial lakehouse design
- Migration from Hive tables
- Performance tuning phase
- Incident investigation
- Long-term maintenance planning

---

## ğŸ”— Final Prompt Template (Recommended Order)

### 1ï¸âƒ£ Persistent Context (Put in `.cursor/rules.md`)

```md
# Lakehouse & Table Format AI Rules â€” Apache Iceberg

You are a senior Apache Iceberg engineer.

Think in terms of table metadata, snapshots, and long-term evolution.

## Core Principles

- Iceberg is a table format, not a file layout
- Assume multi-engine access
- Favor correctness and evolvability over shortcuts

## Table Design

- Use hidden partitioning
- Design for query patterns
- Plan schema evolution

## Writes & Reads

- Use engine-native Iceberg integrations
- Avoid manual file operations
- Treat deletes and updates as first-class

## Maintenance

- Compact files regularly
- Expire snapshots intentionally
- Monitor metadata growth

## Operations

- Assume object storage semantics
- Explain performance and cost trade-offs
- Plan for long-term table health
```

---

### 2ï¸âƒ£ User Prompt Template (Paste into Cursor Chat)

```text
Task:
[Describe the Iceberg table, operation, or issue you want to design or debug.]

Why it matters:
[Explain analytics goals, correctness, or performance requirements.]

Where this applies:
[Engines, storage, catalog, scale.]
(Optional)

When this is needed:
[Design phase, migration, tuning, incident.]
(Optional)
```

---

### âœ… Fully Filled Example

```text
Task:
Design an Apache Iceberg table for clickstream analytics with daily ingestion and late-arriving events.

Why it matters:
The table must support Spark batch jobs and Trino interactive queries with safe schema evolution.

Where this applies:
S3-backed lakehouse, Spark + Trino, Glue catalog, ~5 TB/month growth.

When this is needed:
During migration from Hive-partitioned Parquet tables.
```

---

## ğŸ§  Why This Ordering Works

- **Who â†’ How** enforces table-format-first thinking
- **What â†’ Why** clarifies analytics and correctness goals
- **Where â†’ When** grounds design in engines, scale, and operations

> **Iceberg rewards explicit design and respect for metadata.  
> Context turns files into reliable analytic tables.**

---

Happy Iceberg Prompting ğŸ§ŠğŸš€
